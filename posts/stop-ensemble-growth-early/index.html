<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="feed" href="/feeds/all.atom.xml" title="Articles">

    <title>Stop growing your ensembles early - Tim Head</title>


    <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
    <!-- Bootstrap -->
    <link href="/theme/css/bootstrap.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/default.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      
      ga('create', 'UA-49540142-1', 'betatim.github.io');
      ga('send', 'pageview');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js" type="text/javascript"></script>

    <!-- <script type="text/javascript" src="/interactive/main-built.js" charset="utf-8"></script>-->
    <script type="text/javascript" src="https://cdn.rawgit.com/oreillymedia/thebe/17fe0971303cac24d7e806c8f1bc8ba3c0c40b23/static/main-built.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.10.0/codemirror.min.css">
    <script>
      $(function(){
	      new Thebe({url:"https://tmpnb.org/",
		      kernel_name: "python3",
		      load_mathjax: false,
		      container_selector: "div.entry-content"});
      });
    </script>

    <!-- extra stuff for nbconvert -->
    <!-- extra stuff for nbconvert -->

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/javascript">
      init_mathjax = function() {
      if (window.MathJax) {
        // MathJax loaded
        MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        },
        displayAlign: 'left', // Change this to 'center' to center equations.
        "HTML-CSS": {
          styles: {'.MathJax_Display': {"margin": 0}}
        }
        });
        MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
      }
      }
      init_mathjax();
    </script>

    <link href='https://fonts.googleapis.com/css?family=Roboto:400,400italic,700&subset=latin,latin-ext' rel='stylesheet' type='text/css'>

    <style>
      .cell {
      margin-top: 22px;
      margin-bottom: 11px;
      }
      .text_cell_render {
      padding: 0;
      }
      .title {
      border-left: 7px solid #eee;
      padding: 10px 10px 10px 23px;
      margin-left: -30px;
      }
      .when {
      padding-top: 5px;
      text-transform: uppercase;
      color: #777;
      }
      .entry-content img {
      border-radius: 4px;
      }
      .navbar {
      margin: 75px 0 25px 0;
      }
      .footer {
      margin: 100px 0 25px 0;
      border-top: 1px solid transparent;
      border-color: #eee;
      }
      .powered-by {
      padding: 15px;
      float: right;
      }
      .social {
      padding: 15px;
      float: left;
      }
      .social i {
      padding-left: 15px;
      }
      .social a {
      color: black;
      transition: all 0.25s ease-in;
      -webkit-transition: all 0.25s ease-in;
      }
      .social a:hover {
      text-shadow: 0 0 15px #000;
      }
      .listing span {
      float: right;
      text-transform: uppercase;
      }
      .listing li {
      list-style-type: none;
      }
      .navbar a {
      transition: color 0.5s ease-in;
      -webkit-transition: color 0.5s ease-in;
      }
      .navbar li a {
      font-weight: bold;
      }
      .navbar a {
      text-transform: uppercase
      }
      div.prompt {
      display: none;
      }
      div.thebe_controls {
      margin-top: 5px;
      margin-bottom: 20px;
      }
      div.thebe_controls button {
      background-color: #eaeaea;
      font-size: 1em;
      padding: 5px 20px;
      }
      div.kernel_controls {
      padding: 20px;
      background: rgba(200, 200, 200, 0.4);
      max-width: 600px;
      text-align: center;
      margin: 10px auto;
      }
      div.cell div.thebe_wrap div.input {
      border: 1px solid #c8c8c8;
      padding: 10px;
      }
    </style>

  </head>
  <body>
    <div class="container">
      <div class="row">
	<div class="col-md-8 col-md-offset-2">
	  <nav class="navbar navbar-default" role="navigation">
	    
	    <!-- Brand and toggle get grouped for better mobile display -->
	    <div class="navbar-header">
	      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	      </button>
	      <a class="navbar-brand name" href="http://betatim.github.io/">Tim<b>Head</b></a>
	    </div>
	    <div class="collapse navbar-collapse navbar-right">
	      <ul class="nav navbar-nav">
		<!-- <li><a href="#">Notes</a></li> -->
		<!-- <li><a href="#">Talks</a></li> -->
                         <li><a href="/pages/about.html">
                             About
                         </a></li>
                         <li><a href="/pages/science.html">
                             Science
                         </a></li>
                         <li><a href="/pages/sports.html">
                             Sports
                         </a></li>
	      </ul>
	    </div>
	    
	  </nav>  
	</div>
      </div>
    </div>
    
    <div class="container">
      <div class="col-md-8 col-md-offset-2">
    <section id="content">
        <article>
            <header class="title">
                <h1>
                    <!--<a href="http://betatim.github.io/posts/stop-ensemble-growth-early/"
                       rel="bookmark"
                       title="Permalink to Stop growing your ensembles early">
                        Stop growing your ensembles early
                    </a>-->
		    Stop growing your ensembles early
                </h1>
		<div class="when">01 April 2016</div>
            </header>
            <div class="entry-content">
                <blockquote>
<p>This is an <a href="https://betatim.github.io/posts/really-interactive-posts/">interactive blog post</a>,
you can modify and run the code directly from your browser.
To see any of the output you have to run each of the cells.</p>
</blockquote>
<p>
<div class="cellOOO border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When building an ensemble of trees (a Random Forest or via gradient boosting) one question keeps coming up: how many weak learners should I add to my ensemble?</p>
<p>This post shows you how to keep growing your ensemble until the test error reaches a minimum. This means you do not end up wasting time waiting for your ensemble to build 1000 trees if you only need 200.</p>
<p>First a few imports and generating a toy dataset:</p>

</div>
</div>
</div>
<pre data-executable>
%config InlineBackend.figure_format='retina'
%matplotlib inline
</pre>

<pre data-executable>
import numpy as np

import matplotlib.pyplot as plt

from sklearn.cross_validation import train_test_split
from sklearn.datasets import make_classification
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score
</pre>

<pre data-executable>
X, y = make_classification(n_samples=4000,
                           n_features=40,
                           n_informative=10,
                           n_clusters_per_class=3,
                           random_state=2)
# Split data into a development and evaluation set
X_dev,X_eval, y_dev,y_eval = train_test_split(X, y,
                                              train_size=0.5,
                                              random_state=3)
# Split development set into a train and test set
X_train,X_test, y_train,y_test = train_test_split(X_dev, y_dev,
                                                  test_size=0.33, random_state=4)
</pre>

<div class="cellOOO border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-many-trees?">How many trees?<a class="anchor-link" href="#How-many-trees?">&#182;</a></h2><p>To get started we build an ensemble of 400 trees using gradient boosting. Without
knowing how many trees are needed to reach the minimum test error we can just hope
that 400 is enough. One downside is that we might be waiting a long time to fit a
model with this many trees and then learn that we only needed a small fraction of them (say 200).</p>

</div>
</div>
</div>
<pre data-executable>
opts = dict(max_depth=2, learning_rate=0.1, n_estimators=400)
clf = GradientBoostingClassifier(**opts)
_ = clf.fit(X_train, y_train)
</pre>

<div class="cellOOO border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we plot the validation curve for our fitted classifier and check
with the test set at which number of <code>n_estimators</code> we reach the
minimum test error.</p>

</div>
</div>
</div>
<pre data-executable>
def validation_curve(clf):
    test_score = np.empty(len(clf.estimators_))
    train_score = np.empty(len(clf.estimators_))

    for i, pred in enumerate(clf.staged_predict_proba(X_test)):
        test_score[i] = 1-roc_auc_score(y_test, pred[:,1])

    for i, pred in enumerate(clf.staged_predict_proba(X_train)):
        train_score[i] = 1-roc_auc_score(y_train, pred[:,1])

    best_iter = np.argmin(test_score)
    test_line = plt.plot(test_score, label='test')

    colour = test_line[-1].get_color()
    plt.plot(train_score, '--', color=colour, label='train')

    plt.xlabel("Number of boosting iterations")
    plt.ylabel("1 - area under ROC")
    plt.legend(loc='best')
    plt.axvline(x=best_iter, color=colour)
    
validation_curve(clf)
</pre>

<div class="cellOOO border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As suspected we reach the minimum after approximately 200 iterations. After this the score
$1-\textrm{AUC}(\textrm{ROC})$ does not improve any further. In this case the performance
does not degrade noticably with more iterations but it does take more time to build a model with 400 trees instead of 200.</p>
<p>Now that we know the answer let's see if we can construct a meta estimator that would have
stopped at roughly 200 trees instead of fitting all 400.</p>
<h2 id="Early-stopping">Early stopping<a class="anchor-link" href="#Early-stopping">&#182;</a></h2><p>The obvious solution is to measure the performance of our ensemble as we go along and
stop adding trees once we think we have reached the minimum.</p>
<p>The <code>EarlyStopping</code> meta estimator takes an unfitted estimator, the maximum number
of iterations and a function to calculate the score as arguments.</p>
<p>It will repeatedly add one more base estimator to the ensemble, measure the performance,
and check if we reached minimum. If we reached the minimum it stops, otherwise it
keeps adding base estimators until it reaches the maximum number of iterations.</p>
<p>There are a few more details worth pointing out:</p>
<ul>
<li><p>there is a minimum number of trees required to skip over the noisier part of the score
function; and</p>
</li>
<li><p><code>EarlyStopping</code> does not actually stop at the minimum, instead it continues on until
the score has increased by <code>scale</code> above the current minimum. This is a simple solution
to the problem that we only know we reached the minimum by seeing the score increase
again.</p>
</li>
</ul>

</div>
</div>
</div>
<pre data-executable>
from sklearn.base import ClassifierMixin, clone
from functools import partial


def one_minus_roc(X, y, est):
    pred = est.predict_proba(X)[:, 1]
    return 1-roc_auc_score(y, pred)


class EarlyStopping(ClassifierMixin):
    def __init__(self, estimator, max_n_estimators, scorer,
                 n_min_iterations=50, scale=1.02):
        self.estimator = estimator
        self.max_n_estimators = max_n_estimators
        self.scorer = scorer
        self.scale = scale
        self.n_min_iterations = n_min_iterations
    
    def _make_estimator(self, append=True):
        """Make and configure a copy of the `estimator` attribute.
        
        Any estimator that has a `warm_start` option will work.
        """
        estimator = clone(self.estimator)
        estimator.n_estimators = 1
        estimator.warm_start = True
        return estimator
    
    def fit(self, X, y):
        """Fit `estimator` using X and y as training set.
        
        Fits up to `max_n_estimators` iterations and measures the performance
        on a separate dataset using `scorer`
        """
        est = self._make_estimator()
        self.scores_ = []

        for n_est in range(1, self.max_n_estimators+1):
            est.n_estimators = n_est
            est.fit(X,y)
            
            score = self.scorer(est)
            self.estimator_ = est
            self.scores_.append(score)

            if (n_est > self.n_min_iterations and
                score > self.scale*np.min(self.scores_)):
                return self

        return self
</pre>

<div class="cellOOO border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have all the ingredients, so let's take them for a spin. If <code>EarlyStopping</code> works it should
stop adding trees to the ensemble before we get to 200 trees.</p>

</div>
</div>
</div>
<pre data-executable>
def stop_early(classifier, **kwargs):
    n_iterations = classifier.n_estimators
    early = EarlyStopping(classifier,
                          max_n_estimators=n_iterations,
                          # fix the dataset used for testing by currying
                          scorer=partial(one_minus_roc, X_test, y_test),
                          **kwargs)
    early.fit(X_train, y_train)
    plt.plot(np.arange(1, len(early.scores_)+1),
             early.scores_)
    plt.xlabel("number of estimators")
    plt.ylabel("1 - area under ROC")
    
stop_early(GradientBoostingClassifier(**opts), n_min_iterations=100)
</pre>

<div class="cellOOO border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Success-?!">Success ?!<a class="anchor-link" href="#Success-?!">&#182;</a></h2><p>Looks like it works. The <code>EarlyStopping</code> meta estimator suggests that around 210 trees is
what is needed. While this is not in perfect agreement with what we found before, it is
pretty good given how simple a "minimisation" strategy we use.</p>
<h2 id="Homework">Homework<a class="anchor-link" href="#Homework">&#182;</a></h2><p>Remember we split the dataset into three parts? We reserved one part as evaluation set.
What is it good for? Should you quote the result from the test set as your best estimate
of the performance of your classifier? Try and answer that question using the evaluation
data set. Read my post on <a href="//betatim.github.io/posts/unbiased-performance/">unbiased performance estimates</a> if you need some inspiration.</p>
<h2 id="Taking-if-further">Taking if further<a class="anchor-link" href="#Taking-if-further">&#182;</a></h2><p>There are several ways to build on what we have created so far. Three I can think of:</p>
<ol>
<li>try it with other types of estimators</li>
<li>better methods for dealing with the noise inherent to the score function</li>
<li>deal with very flat score functions</li>
</ol>
<p>One and two are connected. Using <code>EarlyStopping</code> with a <code>RandomForestClassifier</code>
works, however the score as a function of the number of estimators is much more
noisy. Right now this means quite a bit of hand tuning of the minimum number of
iterations and required increase (<code>scale</code>) to declare that we found a minimum.</p>
<p>To get you started, here a little snippet of using <code>EarlyStopping</code> with a random
forest.</p>

</div>
</div>
</div>
<pre data-executable>
from sklearn.ensemble import RandomForestClassifier

stop_early(RandomForestClassifier(n_estimators=400, max_depth=2),
           n_min_iterations=100)
plt.grid()
</pre>

<div class="cellOOO border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you find a mistake or want to tell me something else get in touch on twitter @<a href="//twitter.com/betatim">betatim</a></p>

</div>
</div>
</div></p>
<p>This post started life as a jupyter notebook,
<a href="/downloads/notebooks/early-stopping-trees.ipynb">download it</a>
or
<a href="http://nbviewer.ipython.org/url/betatim.github.io//downloads/notebooks/early-stopping-trees.ipynb">view it</a> online.</p>
            </div>
            <!-- /.entry-content -->
        </article>
    </section>

      </div>
    </div>

<div class="container">
  <div class="row">
    <div class="col-md-8 col-md-offset-2">
      <footer class="footer">
	<div class="social">
	    <a href="https://github.com/betatim"><i class="fa fa-github fa-2x"></i></a>
	    <a href="https://twitter.com/betatim"><i class="fa fa-twitter fa-2x"></i></a>
	    <a href="mailto:betatim@gmail.com"><i class="fa fa-envelope fa-2x"></i></a>
	</div>
	<div class="powered-by">
	  Copyright ©  2014-2016  - Tim Head - <a href="http://docs.getpelican.com/" target="_blank">Pelican</a> powered.
	</div>
      </footer>
    </div>
  </div>
</div>    
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/theme/js/bootstrap.min.js"></script>
  </body>
</html>