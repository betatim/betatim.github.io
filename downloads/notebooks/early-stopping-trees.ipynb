{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping or Smaller Ensembles\n",
    "\n",
    "*This notebook first appeared as a [blog post](//betatim.github.io/posts/bumping) on [Tim Head](//betatim.github.io)'s blog.*\n",
    "\n",
    "*License: [MIT](http://opensource.org/licenses/MIT)*\n",
    "\n",
    "*(C) 2015, Tim Head.*\n",
    "*Feel free to use, distribute, and modify with the above attribution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building an ensemble of trees (like in a Random Forest or gradient boosting) one question keeps coming up: how many base learners should I add to my ensemble?\n",
    "\n",
    "This post shows you how to keep growing your ensemble until the test error reaches a minimum. This means you do not end up wasting time waiting for your ensemble to build 1000 trees if you only need 200.\n",
    "\n",
    "First a few imports and dataset fetching to get us started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget --quiet -O /tmp/HIGGS_background.root http://files.figshare.com/1920208/HIGGS_background.root\n",
    "wget --quiet -O /tmp/HIGGS_signal.root http://files.figshare.com/1920200/HIGGS_signal.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from root_numpy import root2array, rec2array\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading ROOT Trees\n",
    "\n",
    "Data comes first, the excellent [`root_numpy`](//rootpy.github.io/root_numpy/) library makes it easy to read your data stored in a [ROOT](http://root.cern.ch) TTree. Each call to `root2array` will create a 2D array which contains one row per event, and one column representing each branch you want to use.\n",
    "\n",
    "You can download the dataset used in this post from [figshare](//figshare.com):\n",
    " * [HIGGS-background.root](http://figshare.com/articles/HIGGS_background/1314900)\n",
    " * [HIGGS-signal.root](http://figshare.com/articles/HIGGS/1314899)\n",
    " \n",
    "The events are derived from the much larger [HIGGS](http://archive.ics.uci.edu/ml/datasets/HIGGS) dataset. A description of the variables can be found there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the first branch tells you the class of each sample, which is redundant here\n",
    "signal = root2array(\"/tmp/HIGGS_signal.root\")\n",
    "signal = rec2array(signal)[:,1:]\n",
    "\n",
    "backgr = root2array(\"/tmp/HIGGS_background.root\")\n",
    "backgr = rec2array(backgr)[:,1:]\n",
    "\n",
    "# Organise data into one 2D array of shape (n_samples x n_features)\n",
    "X = np.concatenate((signal, backgr))\n",
    "y = np.concatenate((np.ones(signal.shape[0]), np.zeros(backgr.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The big split\n",
    "\n",
    "In addition to loading the data we directly split it into a development and evaluation set. This is good practice in general, here we use it to artificially shrink the dataset and have \"fresh\" copies to evaluate classifier performance on.\n",
    "\n",
    "The development set gets split further into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set aside an evaluation sample\n",
    "# We artificially limit ourselves\n",
    "# to a small subset of the available\n",
    "# samples. This speeds things up and\n",
    "# makes it easier to illustrate things\n",
    "# like over-fitting. In real life you\n",
    "# would not do this.\n",
    "X_dev,X_eval, y_dev,y_eval = train_test_split(X, y,\n",
    "                                              train_size=20000,\n",
    "                                              test_size=10000,\n",
    "                                              random_state=42)\n",
    "\n",
    "# Split development set into a train and test sample\n",
    "X_train,X_test, y_train,y_test = train_test_split(X_dev, y_dev,\n",
    "                                                  test_size=0.33, random_state=4685)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(max_depth=4,\n",
    "                                 learning_rate=0.2,\n",
    "                                 n_estimators=200)\n",
    "_ = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validation_curve(clf):\n",
    "    test_score = np.empty(len(clf.estimators_))\n",
    "    train_score = np.empty(len(clf.estimators_))\n",
    "\n",
    "    for i, pred in enumerate(clf.staged_predict_proba(X_test)):\n",
    "        test_score[i] = 1-roc_auc_score(y_test, pred[:,1])\n",
    "\n",
    "    for i, pred in enumerate(clf.staged_predict_proba(X_train)):\n",
    "        train_score[i] = 1-roc_auc_score(y_train, pred[:,1])\n",
    "\n",
    "    best_iter = np.argmin(test_score)\n",
    "    test_line = plt.plot(test_score)\n",
    "\n",
    "    colour = test_line[-1].get_color()\n",
    "    plt.plot(train_score, '--', color=colour)\n",
    "\n",
    "    plt.xlabel(\"Number of boosting iterations\")\n",
    "    plt.ylabel(\"1 - area under ROC\")\n",
    "    plt.axvline(x=best_iter, color=colour)\n",
    "    \n",
    "validation_curve(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import ClassifierMixin, clone\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def one_minus_roc(X, y, est):\n",
    "    pred = est.predict_proba(X)[:, 1]\n",
    "    return 1-roc_auc_score(y, pred)\n",
    "\n",
    "\n",
    "class EarlyStopping(ClassifierMixin):\n",
    "    def __init__(self, estimator, max_n_estimators, scorer, scale=1.01):\n",
    "        self.estimator = estimator\n",
    "        self.max_n_estimators = max_n_estimators\n",
    "        self.scorer = scorer\n",
    "        self.scale = scale\n",
    "    \n",
    "    def _make_estimator(self, append=True):\n",
    "        \"\"\"Make and configure a copy of the `estimator` attribute.\"\"\"\n",
    "        estimator = clone(self.estimator)\n",
    "        estimator.n_estimators = 1\n",
    "        estimator.warm_start = True\n",
    "        return estimator\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        est = self._make_estimator()\n",
    "        self.scores_ = []\n",
    "\n",
    "        for n_est in xrange(1, self.max_n_estimators+1):\n",
    "            est.n_estimators = n_est\n",
    "            est.fit(X,y)\n",
    "            \n",
    "            score = self.scorer(est)\n",
    "            self.estimator_ = est\n",
    "            self.scores_.append(score)\n",
    "\n",
    "            if score > self.scale*np.min(self.scores_):\n",
    "                return self\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stop_early(classifier, scale=1.01):\n",
    "    early = EarlyStopping(classifier,\n",
    "                          max_n_estimators=200,\n",
    "                          scale=scale,\n",
    "                          scorer=partial(one_minus_roc, X_test, y_test))\n",
    "    early.fit(X_train, y_train)\n",
    "    plt.plot(np.arange(1, 200)[:len(early.scores_)],\n",
    "             early.scores_)\n",
    "    plt.xlabel(\"number of estimators\")\n",
    "    plt.ylabel(\"1 - area under ROC\")\n",
    "    \n",
    "stop_early(GradientBoostingClassifier(learning_rate=0.2, max_depth=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "stop_early(RandomForestClassifier(max_depth=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "stop_early(BaggingClassifier(DecisionTreeClassifier(max_depth=4)), scale=1.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
